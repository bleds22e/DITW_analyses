---
title: "Data in the Wild--Analyses"
author: "Ellen Bledsoe"
date: today
format: gfm
---

This document contains the analyses for the Data in the Wild course.

## Packages and Data

Load the packages used in analyses.

```{r}
#| message: false
library(tidyverse)    # for data wrangling
library(lme4)         # for running linear mixed effects models
library(lmerTest)     # for getting p-values from lme4 models
library(broom.mixed)  # for tidy summaries of lme4 models
```

Read in the cleaned dataset.

```{r}
#| message: false
data <- read_csv("../data_clean/data_cleaned.csv")

head(data)
```

## Check Distributions

Look at the distribution of responses by question.

```{r}
data %>% 
  # reshape the data for plotting
  pivot_longer(starts_with("Q"), 
               names_to = "Question", 
               values_to = "Value") %>% 
  # reorder "pre" and "post" for plotting
  mutate(Survey_Type = as_factor(Survey_Type)) %>% 
  # pipe into a histogram
  ggplot(aes(Value)) +
  geom_histogram(aes(fill = Survey_Type), 
                 position = "identity", alpha = 0.5, bins = 5) +
  facet_wrap(~ Question, scales = "free_x") +
  labs(x = "Score",
       y = "Frequency",
       fill = "Survey") +
  theme_bw()
```

## Run Linear Mixed Effects Models

We want to know if the answers to each question have changed significantly from pre- to post-survey.

I'm using a linear mixed effects model to include the student's identity, the university where the course was taught, and the semester in which the course was taught as random effects.

### Question 39

Run a model for Q39 responses as a function of the survey (pre vs. post). Include student, semester, and university as random effects.

```{r}
model_Q39 <- lmer(Q39 ~ Survey_Type + (1 | Term) + (1 | School) + (1 | StudentID), 
              data = data)
(output_Q39 <- tidy(model_Q39))
```

Semester (`Term`) has a variance of 0, which is causing singularity. Let's remove that random effect.

```{r}
model_Q39 <- lmer(Q39 ~ Survey_Type + (1 | School) + (1 | StudentID), 
              data = data)
(output_Q39 <- tidy(model_Q39))
```

This removes warning about singularity, which is great.

> **NOTE:** The p-value is `r round(output_Q39$p.value[2], 2)`, meaning no significant difference.

Let's check the model performance. Q39 answers do not have a normal distribution.

```{r include=FALSE}
par(mfrow = c(1,2))

resid_vals <- residuals(model_Q39)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")

par(mfrow = c(1,1))
```

Residuals aren't great, but aren't horrible, either?

Out of curiosity, what happens when we include the school as a fixed effect? We actually have reason to believe this might make more sense--one class was more computationally focused while another class was more statistically focused

```{r}
model_Q39 <- lmer(Q39 ~ Survey_Type + School + (1 | StudentID), 
              data = data)
broom.mixed::tidy(model_Q39)
```

This does not change out qualitative result at all. 

However, it does indicate that the institution at which the course was taught does have an impact on the scores. I'm not sure if this is worth investigating, though it does make some sense. The two schools have quite different student bodies, and the courses had slightly different flavors--one was more stats-heavy and the other more computationally-focused.

### Question 15

Let's run the analysis with Question 15. For every question, the variance of semester ends up being 0, so I will preemptively remove it.

```{r}
model_Q15 <- lmer(Q15 ~ Survey_Type + (1 | StudentID) + (1 | School), 
                  data = data)
(output_Q15 <- tidy(model_Q15))
```

Check the residuals

```{r include=FALSE}
par(mfrow = c(1,2))
resid_vals <- residuals(model_Q15)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")
par(mfrow = c(1,1))
```

> **NOTE:** The p-value is `r round(output_Q15$p.value[2], 2)`, meaning no significant difference.

### Question 17_1

Let's run the analysis with Question 17_1.

```{r}
model_Q17_1 <- lmer(Q17_1 ~ Survey_Type + (1 | StudentID) + (1 | School), 
                  data = data)
(output_Q17_1 <- tidy(model_Q17_1))
```

Check the residuals

```{r include=FALSE}
par(mfrow = c(1,2))
resid_vals <- residuals(model_Q17_1)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")
par(mfrow = c(1,1))
```

> **NOTE:** The p-value is `r round(output_Q17_1$p.value[2], 2)`, meaning no significant difference.

### Question 17_2

```{r}
model_Q17_2 <- lmer(Q17_2 ~ Survey_Type + (1 | StudentID) + (1 | School), 
                  data = data)
(output_Q17_2 <- tidy(model_Q17_2))
```

Check the residuals

```{r include=FALSE}
par(mfrow = c(1,2))
resid_vals <- residuals(model_Q17_2)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")
par(mfrow = c(1,1))
```

> **NOTE:** The p-value is `r round(output_Q17_2$p.value[2], 2)`, meaning no significant difference.

### Question 17_3

Let's run the analysis with Question 17_3.

This is the question about the importance of data science in conservation.

```{r}
model_Q17_3 <- lmer(Q17_3 ~ Survey_Type + (1 | StudentID) + (1 | School), 
                  data = data)
(output_Q17_3 <- tidy(model_Q17_3))
```

Check the residuals

```{r include=FALSE}
par(mfrow = c(1,2))
resid_vals <- residuals(model_Q17_3)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")
par(mfrow = c(1,1))
```

> **NOTE:** The p-value is `r round(output_Q17_3$p.value[2], 2)`, which is our only significant result.

### Question 17_4

Let's run the analysis with Question 17_4.

```{r}
model_Q17_4 <- lmer(Q17_4 ~ Survey_Type + (1 | StudentID) + (1 | School), 
                  data = data)
(output_Q17_4 <- tidy(model_Q17_4))
```

Check the residuals

```{r include=FALSE}
par(mfrow = c(1,2))
resid_vals <- residuals(model_Q17_4)
qqnorm(resid_vals); qqline(resid_vals)
hist(resid_vals, main = "Histogram of Residuals", xlab = "Residuals")
par(mfrow = c(1,1))
```

> **NOTE:** The p-value is `r round(output_Q17_4$p.value[2], 2)`, meaning no significant difference.

## Results

First, bring all of the results together.

```{r}
results <- list_rbind(mget(ls(pattern = "output_*")), names_to = "id") |> 
  filter(term == "Survey_TypePre") |> 
  mutate(id = str_extract(id, "Q.*")) |> 
  select(id, term:p.value)
```

Now correct for multiple statistical tests.

```{r}
results <- results |> 
  mutate(p.adjust = p.adjust(p.value, method = "BH"))
```

With the correction for multiple tests, no p-values are considered significant.

```{r}
results |> 
  select(id, p.value, p.adjust)
```
